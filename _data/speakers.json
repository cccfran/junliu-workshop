[
  {
    "id": 0,
    "first": "",
    "last": "",
    "time": "8:30am",
    "title": "Breakfast"
  },
  {
    "id": 1,
    "time": "9:00am",
    "info": "session",
    "first": "Samuel",
    "last": "Kou",
    "title": "Opening Remarks",
    "url": "https://statistics.fas.harvard.edu/people/samuel-kou",
    "ins": "Harvard University"
  },
  {
    "id": 2,
    "time": "9:10–10:10am",
    "info": "session",
    "first": "Rong",
    "last": "Chen",
    "title": "Bayesian-inspired Methods in AI and Statistics",
    "abstract": "",
    "url": "https://stat.rutgers.edu/people-pages/faculty/people/371-rong-chen",
    "ins": "Rurgers University",
    "organizer": "Samuel Kou"
  },
  {
    "id": 3,
    "time": "",
    "first": "Yingnian",
    "last": "Wu",
    "title": "Latent Thought Language Models with<br> Variational Bayes Inference-Time Computation",
    "abstract": "We propose a novel class of language models, Latent Thought Models (LTMs), which incorporate explicit latent thought vectors that follow an explicit prior model in latent space. These latent thought vectors guide the autoregressive generation of ground tokens through a Transformer decoder. Training employs a dual-rate optimization process within the classical variational Bayes framework. Empirical studies reveal that LTMs possess additional scaling dimensions beyond traditional Large Language Models (LLMs), such as the number of iterations in inference-time computation and number of latent thought vectors. Higher sample efficiency can be achieved by increasing training compute per token, with further gains possible by trading model size for more inference steps. Designed based on these scaling properties, LTMs demonstrate superior sample and parameter efficiency compared to autoregressive models and discrete diffusion models. They significantly outperform these counterparts in validation perplexity and zero-shot language modeling tasks. Additionally, LTMs exhibit emergent few-shot in-context reasoning capabilities that scale with model size, and achieve competitive performance in conditional and unconditional text generation. <br><br> The project page is available at <a href=\"https://deqiankong.github.io/blogs/ltm\">https://deqiankong.github.io/blogs/ltm</a>. This is a collaboration between UCLA and Lambda.",
    "ins": "University of California, <br> Los Angeles",
    "url": "http://www.stat.ucla.edu/~ywu/"
  },
  {
    "id": 4,
    "time": "",
    "first": "Shane",
    "last": "Jensen",
    "title": "Bayesian Non-Parametric Clustering at Micro and Macro Scales",
    "abstract": "I will discuss Jun Liu's substantial impact on Bayesian nonparametrics and clustering through the widespread use of the Dirichlet process.   I will present two applications of the Dirichlet process (DP), starting with early work on clustering on the micro level of DNA sequence data in order to find conserved patterns in genomic sequences.  I will then discuss more recent work on DP-based clustering of crime at the macro level of Philadelphia neighborhoods.  Finally, I will bring the micro and macro closer together in an extension of our DP-based approach to clustering crime across multiple levels of resolution simultaneously.",
    "ins": "University of Pennsylvania",
    "url": "https://statistics.wharton.upenn.edu/profile/stjensen/"
  },
  {
    "id": 5,
    "time": "",
    "first": "Xinran",
    "last": "Li",
    "title": "Bootstrap Inference for Partially Identified Parameters <br> with Unobserved Variables and Over-identification",
    "abstract": "Introducing auxiliary data or parameters can significantly facilitate Bayesian computation, while treating known parameters as unknown can, in some cases, improve Frequentist inference. In this talk, I will present a bootstrap approach inspired by these ideas for conducting inference on partially identified parameters in the presence of missing data. Specifically, we focus on parameters defined by a set of over-identified estimating equations involving unobserved or missing variables subject to general constraints. An important application and the main motivation for this framework is sensitivity analysis for unmeasured confounding in observational studies.<br><br>In contrast to settings without missing data, we choose not to reduce the number of estimating equations by combining them, since each equation may provide useful information for constraining the unobserved variables. However, over-identification, meaning there are more equations than parameters, creates challenges for existing bootstrap methods, which may fail to produce valid confidence sets. To address this issue, we propose a bootstrap procedure that introduces auxiliary data and parameters. While these auxiliary components can be conceptual and do not affect the actual implementation, they play a critical role in establishing the validity of the proposed inference approach.",
    "ins": "University of Chicago",
    "url": "https://sites.google.com/view/xinranli"
  },
  {
    "id": 6,
    "time": "10:10–10:30am",
    "title": "Break"
  },
  {
    "id": 7,
    "info": "session",
    "time": "10:30–11:30am",
    "first": "Haiyan",
    "last": "Huang",
    "title": "Advancing Biological Discovery and Therapeutics <br> through AI and Statistical Modeling",
    "abstract": "",
    "url": "https://statistics.berkeley.edu/people/haiyan-huang",
    "ins": "University of California, <br>Berkeley",
    "organizer": "Tianxi Cai, Xihong Lin"
  },
  {
    "id": 8,
    "time": "",
    "first": "Hongyu",
    "last": "Zhao",
    "title": "Modeling and Predicting Single-Cell Multi-Gene Perturbation Responses",
    "abstract": "Understanding cellular responses to genetic perturbations is essential for deciphering gene regulation and phenotype formation. While high-throughput single-cell RNA-sequencing has facilitated detailed profiling of heterogeneous transcriptional responses to perturbations at the single-cell level, there remains a pressing need for computational models that can decode the mechanisms driving these responses and accurately predict outcomes to prioritize target genes for experimental design. We will introduce a deep generative learning framework designed to model and predict single-cell transcriptional responses to genetic perturbations, including single-gene and combinatorial multi-gene perturbations. Our method can effectively integrate prior biological knowledge and disentangle basal cell states from perturbation-specific salient representations by leveraging gene embeddings derived from large language models. Through comprehensive evaluations on multiple single-cell CRISPR Perturb-seq datasets, our method outperformed state-of-the-art methods in predicting perturbation outcomes, achieving higher prediction accuracy. Notably, it demonstrated robust generalization to unseen target genes and perturbations, and its predictions captured both average expression changes and the heterogeneity of single-cell responses. Furthermore, its predictions enable diverse downstream analyses, including identifying differentially expressed genes and exploring genetic interactions, demonstrating its utility and versatility. This is joint work with Gefei Wang, Tianyu Liu, Jia Zhao, and Youshu Cheng.",
    "ins": "Yale University",
    "url": "https://ysph.yale.edu/profile/hongyu_zhao/"
  },
  {
    "id": 9,
    "time": "",
    "first": "Chiara",
    "last": "Sabatti",
    "title": "When Is a Variable Important?",
    "abstract": "A common characteristics of genomic studies is that they measure a large number of variables. And it is typically of interest to learn which, among these many, is relevant  for an outcome.  What does it mean for a variable to be \"important\"? How can we capture the fact that the relevance of explanatory variables might change across different contexts? I will describe some recent progress in this area.",
    "ins": "Stanford University",
    "url": "https://statistics.stanford.edu/people/chiara-sabatti"
  },
  {
    "id": 10,
    "time": "",
    "first": "Shirley",
    "last": "Liu",
    "title": "AI-driven Next-generation Cancer Biotherapeutics",
    "abstract": "GV20 Therapeutics is a clinical-stage, AI-driven biotherapeutics company focused on next-generation cancer treatments. The company’s lead program in development as a potential cancer immunotherapy is GV20-0251—a monoclonal antibody targeting the novel immune checkpoint IGSF8. It is the first AI-designed antibody against an AI-predicted target to enter clinical trials for treating advanced solid tumors. GV20’s proprietary STEAD AI platform enabled the program to advance from target discovery to investigational new drug (IND) approval in just three years, with early clinical data showing a favorable safety profile and promising efficacy in patients with advanced metastatic cancer.",
    "ins": "GV20 Therapeutics / <br> Dana-Farber Cancer Institute",
    "url": "https://liulab-dfci.github.io"
  },
  {
    "id": 11,
    "time": "11:30am–1:00pm",
    "title": "Lunch, Group Photo"
  },
  {
    "id": 12,
    "time": "12:10–12:50pm",
    "info": "session",
    "title": "From Spark to Signal: Lightning Talks on <br> Data Science, AI, and Statistics for Knowledge Discovery",
    "first": "Ping",
    "last": "Ma",
    "url": "https://www.stat.uga.edu/directory/people/ping-ma",
    "ins": "University of Georgia",
    "organizer": "Ping Ma"
  },
  {
    "id": 13,
    "info": "session",
    "time": "1:00–2:00pm",
    "first": "Linda",
    "last": "Zhao",
    "title": "Keynote",
    "url": "https://statistics.wharton.upenn.edu/profile/lzhao/",
    "ins": "University of Pennsylvania"
  },
  {
    "id": 13.5,
    "time": "",
    "info": "keynote",
    "first": "Augustine",
    "last": "Kong",
    "title": "Some Reflections on How the Field of Statistics Has Changed <br> in the Last Forty Years and the Role of Chinese Students",
    "url": "https://www.bdi.ox.ac.uk/Team/augustine-kong",
    "ins": "University of Oxford"
  },
  {
    "id": 14,
    "time": "2:00–2:30pm",
    "title": "Break"
  },
  {
    "id": 15,
    "info": "session",
    "time": "2:30–3:30pm",
    "first": "Xuming",
    "last": "He",
    "title": "AI in Action: From Complex Data to Credible Discovery",
    "abstract": "",
    "ins": "Washington University <br> in St. Louis",
    "url": "https://sds.wustl.edu/people/xuming-he",
    "organizer": "Linda Zhao"
  },
  {
    "id": 16,
    "time": "",
    "first": "Tony",
    "last": "Cai",
    "title": "Transcending Data Boundaries: <br> Transfer Knowledge in Statistical Learning",
    "abstract": "In this talk, I will present results on minimax and adaptive transfer learning for nonparametric classification under a posterior drift model with distributed differential privacy constraints. We first establish the minimax misclassification rate, characterizing how privacy constraints and both source and target samples influence classification accuracy. The results reveal phase transition phenomena and highlight fundamental trade-offs between privacy and accuracy. Building on this, we propose a data-driven adaptive classifier that achieves the optimal rate (up to a logarithmic factor) over a broad class of parameter spaces, while respecting the same privacy constraints.",
    "ins": "University of Pennsylvania",
    "url": "https://statistics.wharton.upenn.edu/profile/tcai/"
  },
  {
    "id": 17,
    "time": "",
    "first": "Edoardo",
    "last": "Airoldi",
    "title": "Valid Statistical Analyses and Reproducible Science <br> in the Era of High-throughput Biology",
    "abstract": "High-throughput technology (eg, sequencing, mass spec) allows us to quantify biological mechanisms at a resolution that array technology and small scale experiments cannot. Currently, a substantial portion of biological research is leveraging some of these technologies. This flexibility comes with a price, however. Modern high-throughput instrumentation relies on built-in data collection protocols that are often biased. (For instance, a mass spec selects the most abundant ions, at an early stage of the measurement process, for further analysis.) The major unexpected consequence of such protocols is that they carry information about those quantities we are interested in estimating (absolute protein abundance, in the mass spec example). Scientists that do not account for this information during analysis, whether by counting or estimation using a statistical model, will likely base their scientific conclusions on misleading numbers, even in simple experimental conditions. This statistical issue is poorly understood by practitioners and amateur statisticians alike. It is arguably the main challenge we need to tackle to produce valid scientific conclusions in the era of high-throughput technology. I'll provide an illustration in mass spectrometry.",
    "ins": "Temple University",
    "url": "https://www.fox.temple.edu/directory/edo-airoldi-tuj91381"
  },
  {
    "id": 18,
    "time": "",
    "first": "Qing",
    "last": "Zhou",
    "title": "Causal Discovery on Dependent Data",
    "abstract": "Learning causal structures from observational data is a central challenge in statistics and machine learning. A common assumption in causal discovery is that data samples are i.i.d., yet this assumption is often violated in real-world settings such as social networks, brain imaging, and cell differentiation, where dependencies exist across observational units. In this talk, I present a unified framework for causal structure learning from dependent data through decorrelation. For continuous data, we extend the Gaussian DAG model by incorporating sample dependence over an undirected graph. For binary data, we introduce a latent utility model with correlated errors across units, under a probit regression-based DAG formulation. In both cases, we develop methods to estimate the between-unit covariance structure and use it to decorrelate the data, enabling the application of standard DAG learning algorithms. Numerical experiments on both simulated and real datasets show that these decorrelation methods significantly improve the accuracy of causal graph recovery compared to directly applying standard methods to dependent data.",
    "ins": "University of California, <br> Los Angeles",
    "url": "http://www.stat.ucla.edu/~zhou/"
  },
  {
    "id": 19,
    "info": "session",
    "time": "3:30–4:15pm",
    "first": "Tianxi",
    "last": "Cai",
    "title": "Panel Discussion: <br> AI Meets Statistics — A Two-Way Power Boost",
    "url": "https://dbmi.hms.harvard.edu/people/tianxi-cai",
    "ins": "Harvard University"
  },
  {
    "id": 20,
    "info": "panel",
    "panelist": [
    "Xiao-Li Meng",
    "Susan Murphy",
    "Jamie Robins",
    "Yuan Yuan"
  ],
  "urls": [
    "https://statistics.fas.harvard.edu/people/xiao-li-meng",
    "https://statistics.fas.harvard.edu/people/susan-murphy",
    "https://hsph.harvard.edu/profile/james-m-robins/",
    "https://sites.google.com/site/danioyuan/Home"
  ],
  "inss": [
    "Harvard University",
    "Harvard University",
    "Harvard University",
    "Google"
  ],
  },
  {
    "id": 21,
    "time": "4:15pm",
    "first": "Xihong",
    "last": "Lin",
    "title": "Closing Remarks",
    "url": "https://hsph.harvard.edu/profile/xihong-lin/",
    "ins": "Harvard University"
  },
  {
    "id": 12.1,
    "first": "Haiyan",
    "last": "Huang",
    "ins": "University of California,<br> Berkeley",
    "title": "A Journey from Mathematics to Genomic Data Analysis",
    "abstract": "I will share a line of my research that led me from pure mathematics into genomic data analysis, navigating challenges at the intersection of statistics, biology, and data science. I will also reflect on how Jun has influenced and inspired me throughout this journey.",
    "url": "https://statistics.berkeley.edu/people/haiyan-huang"
  },
  {
    "id": 12.2,
    "first": "Leo",
    "last": "Guo",
    "ins": "Bayesian Investment",
    "title": "More Than Numbers",
    "abstract": "In these brief remarks, I hope to share a few personal reflections on what an honor it is to celebrate Professor Jun Liu's two incredible milestones. I'll begin with the surreal and often humorous feeling of returning to campus after twelve years, recalling some of the shared struggles—like agonizing over a stubborn statistical model—that shaped us in his lab. From there, I want to focus on two profound gifts I received from his mentorship: the invaluable lesson in resilience, and the powerful philosophy that statistics is much more than numbers, but a way of seeing the world. I'll briefly touch on how these principles became my compass long after I left academia, guiding me through the challenges of the startup world. Ultimately, this is simply my heartfelt way of saying thank you, and of celebrating the incredible, lasting impact of a great mentor.",
    "url": "https://www.linkedin.com/in/leiguosf/"
  },
  {
    "id": 12.3,
    "first": "Jinfeng",
    "last": "Zhang",
    "ins": "Insilicom LLC",
    "title": "A Comprehensive Large-Scale Biomedical Knowledge Graph <br> for AI-Powered Data-Driven Biomedical Research",
    "abstract": "To address the rapid growth of scientific publications and data in biomedical research, knowledge graphs (KGs) have become a critical tool for integrating large volumes of heterogeneous data to enable efficient information retrieval and automated knowledge discovery. However, transforming unstructured scientific literature into KGs remains a significant challenge, with previous methods unable to achieve human-level accuracy. Here we used an information extraction pipeline that won first place in the LitCoin Natural Language Processing Challenge (2022) to construct a large-scale KG named iKraph using all PubMed abstracts. The extracted information matches human expert annotations and significantly exceeds the content of manually curated public databases. To enhance the KG’s comprehensiveness, we integrated relation data from 40 public databases and relation information inferred from high-throughput genomics data. This KG facilitates rigorous performance evaluation of automated knowledge discovery, which was infeasible in previous studies. We designed an interpretable, probabilistic-based inference method to identify indirect causal relations and applied it to real-time COVID-19 drug repurposing from March 2020 to May 2023. Our method identified around 1,200 candidate drugs in the first 4 months, with one-third of those discovered in the first 2 months later supported by clinical trials or PubMed publications. These outcomes are very challenging to attain through alternative approaches that lack a thorough understanding of the existing literature. A cloud-based platform (<a href=\"https://biokde.insilicom.com\">https://biokde.insilicom.com</a>) was developed for academic users to access this rich structured data and associated tools.",
    "url": "https://ani.stat.fsu.edu/~jinfeng/"
  },
  {
    "id": 12.4,
    "first": "Huimin",
    "last": "Cheng",
    "ins": "Boston University",
    "title": "JADE: Joint Alignment and Deep Embedding <br> for Multi-Slice Spatial Transcriptomics",
    "abstract": "As spatially resolved transcriptomics (SRT)  datasets increasingly span multiple adjacent or replicated slices, effective joint analysis across slices is needed to reconstruct tissue structures and identify consistent spatial gene expression patterns. This requires resolving spatial correspondences between slices while capturing shared transcriptomic features, two tasks that are typically addressed in isolation. Multi-slice analysis remains challenging due to physical distortions, technical variability, and batch effects. To address these challenges, we introduce Joint Alignment and Deep Embedding for multi-slice SRT (JADE), a unified computational framework that simultaneously learns spatial location-wise alignments and shared low-dimensional embeddings across tissue slices. Unlike existing methods, JADE adopts a roundtrip framework in which each iteration alternates between alignment and embedding refinement. To infer alignment, we employ attention mechanisms that dynamically assess and weight the importance of different embedding dimensions, allowing the model to focus on the most alignment-relevant features while suppressing noise.  To the best of our knowledge, JADE is the first method that jointly optimizes alignment and representation learning in a shared latent space, enabling robust multi-slice integration. We demonstrate that JADE outperforms existing alignment and embedding methods across multiple evaluation metrics in the 10x Visium human dorsolateral prefrontal cortex (DLPFC) and Stereo-seq axolotl brain datasets. By bridging spatial alignment and feature integration, JADE provides a scalable and accurate solution for cross-slice analysis of SRT data.",
    "url": "https://www.bu.edu/sph/profile/huimin-cheng/"
  },
  {
    "id": 12.5,
    "first": "Steve",
    "last": "Qin",
    "ins": "Emory University",
    "title": "Adaptive / Bayesian Hierarchical Model and Its Applications",
    "abstract": "Bayesian hierarchical model is a powerful tool that enables borrowing strength among exchangeable experiments. However, inference results are not reliable if the exchangeable condition is compromised.  We proposed some adaptive strategy to overcome this challenge. We showed its applications in analyzing high-throughput genomics data.",
    "url": "https://winshipcancer.emory.edu/profiles/qin-zhaohui-steve.php"
  },
  {
    "id": 12.6,
    "first": "Xin",
    "last": "Xing",
    "ins": "Virginia Tech",
    "title": "The Evolution of Statistical Inference: From Classical P-values <br> Through Mirror Statistics to Modern Generative Models",
    "abstract": "In this talk, I will trace the remarkable evolution of statistical inference from its classical foundations to cutting-edge generative approaches. We begin with the classical p-value framework that established the theoretical bedrock of 20th-century statistics, then explore how mirror statistics and data-splitting methods emerged to address the multiple testing crisis in high-dimensional settings by cleverly exploiting distributional symmetries. The talk culminates with modern flow-based predictive inference methods that revolutionize uncertainty quantification by learning flexible data representations, enabling robust inference even when traditional exchangeability assumptions fail and testing data is contaminated. I will show how these three parts complement each other in practice, forming a unified toolkit that empowers practitioners to tackle diverse inference challenges in high-dimensional and complex data.",
    "url": "https://www.stat.vt.edu/people/stat-faculty/xing-xin.html"
  },
  {
    "id": 12.7,
    "first": "Junni",
    "last": "Zhang",
    "ins": "Peking University",
    "title": "Fast, Scalable Bayesian Demography",
    "abstract": "Abstract: There is a growing demand for disaggregated demographic estimates to support both analysis and policymaking, driving the use of increasingly available, highly detailed demographic data. However, such data are often sparse or noisy. To address this, we combine mathematical demography with Bayesian statistics to leverage similar—but not identical—demographic patterns and to effectively handle missing data. We have developed a fast and scalable R package that implements a general family of Bayesian models, making these methods both accessible and practical for demographers.",
    "url": "https://en.nsd.pku.edu.cn/faculty/fulltime/z/496245.htm"
  },
  {
    "id": 12.8,
    "first": "Yuguo",
    "last": "Chen",
    "ins": "University of Illinois Urbana-Champaign",
    "title": "Sampling Sequential Importance Sampling",
    "abstract": "Sequential importance sampling (SIS) is a versatile and powerful Monte Carlo method for tackling complex statistical inference problems. It has had a profound impact on modern statistical computation, thanks to Professor Jun Liu’s pioneering contributions. In this talk, I will share a few projects related to SIS that I was fortunate to work on with Professor Liu and my students.",
    "url": "https://stat.illinois.edu/directory/profile/yuguo"
  },
  {
    "id": 12.9,
    "first": "Roee",
    "last": "Gutman",
    "ins": "Brown University",
    "title": "Record Linkage as a Data Science Challenge",
    "abstract": "Linking data from various sources is essential for generating accurate statistics and assessing the impact of interventions. With declining survey response rates, rapid technological advancements, and the growing volume of data collected by different organizations the demand for robust record linkage methods continues to grow. A major challenge in the linkage process arises when unique identifying information is unavailable across datasets, because of privacy concerns or errors in recording. To address this challenge there is a need for efficient computational algorithms and for statistical inference tools that adjust for false matches and missed links. We view record linkage as a missing data problem and develop Bayesian procedures that enhance the linkage results while ensuring valid statistical inferences.",
    "url": "https://vivo.brown.edu/display/rg5"
  },
  {
    "id": 12.91,
    "first": "Mayetri",
    "last": "Gupta",
    "ins": "University of Glasgow",
    "title": "Bayesian Hierarchical Latent Variable-based Modelling <br> for Large and Complex Genomic Datasets",
    "abstract": "Advances in genomic sequencing technologies in the past few decades have opened up the possibility of making previously unthinkable biological discoveries at extremely high resolution- but have led to numerous challenging problems in how to sensibly and accurately analyse the generated data. These data are typically of huge dimension and/or large volumes; may be discrete in nature; are subject to various artefacts; and their distributions exhibit non-standard features, such as long-ranging correlations, non-ellipsoidal shapes, skewness and multimodality- which may cause difficulties in making successful inference through usual statistical modelling approaches. In this lightning talk, I will highlight some recent examples of work from our group where detail-oriented Bayesian modelling coupled with robust, efficient and powerful Monte Carlo-based computational methods enabled useful inference and meaningful biological discoveries. These include (i) clustering high volumes of non-ellipsoidal data from genotype sequencing experiments; (ii) detecting differential epigenetic profiles from high-throughput DNA methylation data and (iii) discovering groups of genetic variants associated with complex phenotypic traits in humans. This talk is based on joint work with Edoardo Redivo, Hien D. Nguyen, Huizi Zhang, Ben Swallow, Tushar Ghosh, Lanxin Li, Vincent Macaulay, Peter D. Adams, members of the Framingham Bone research consortium and the BHF Institute of Cardiovascular & Medical Sciences at Glasgow.",
    "url": "https://www.maths.gla.ac.uk/~mgupta/"
  }
]
